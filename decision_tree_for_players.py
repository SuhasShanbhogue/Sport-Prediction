# -*- coding: utf-8 -*-
"""Decision Tree for players.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VrKHv1_DqnRr0XUwcIe_zrSfL1mSOZKG
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.decomposition import PCA
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
 
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import roc_curve
from matplotlib import pyplot
import seaborn as sn
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import plot_confusion_matrix

p_2011 = pd.read_csv("../data/Players_final_2011.csv")
p_2012 = pd.read_csv("../data/Players_final_2012.csv")
p_2013 = pd.read_csv("../data/Players_final_2013.csv")
p_2014 = pd.read_csv("../data/Players_final_2014.csv")
p_2015 = pd.read_csv("../data/Players_final_2015.csv")

def plot_pruning_graph(X_train, y_train, clf):
    # clf is the model you are using
    # Would be declared in the main function like :
    # clf = DecisionTreeClassifier(random_state=0, criterion="entropy", ccp_alpha=0.01,
    #                              class_weight='balanced',
    #                              splitter = "random"
    path = clf.cost_complexity_pruning_path(X_train, y_train)
    ccp_alphas, impurities = path.ccp_alphas, path.impurities
    fig, ax = plt.subplots()
    ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post", color = 'black', ls = '--')
    ax.set_xlabel("effective alpha")
    ax.set_ylabel("total impurity of leaves")
    ax.set_title("Total Impurity vs effective alpha for training set")
    #Gets stored
    fig.savefig("ImpurityVSalpha.png")

def metrics_combined(comb_y_true,comb_y_pred,comb_y_probs):
  roc =0
  f1score =0
  roc_l=[]
  f1_score_l=[]
  cf_total=0
  for i in range(4):
    roc += roc_auc_score(comb_y_true[i],comb_y_probs[i])
    roc_l.append(roc_auc_score(comb_y_true[i],comb_y_probs[i]))
    f1score += f1_score(comb_y_true[i],comb_y_pred[i])
    f1_score_l.append(f1_score(comb_y_true[i],comb_y_pred[i]))
    cf = confusion_matrix(comb_y_true[i],comb_y_pred[i])
    cf_total += cf
  
  print("Mean ROC score:{}".format(roc/4))
  print("Mean F1 score{}".format(f1score/4))
  print("Confusion Matrix for all:")
  print(cf_total)
 
  
  x = ["2012", "2013", "2014", "2015"] 
  plt.plot(x,roc_l,label='ROC', linestyle = '--', color = 'black')
  plt.plot(x,f1_score_l,label='F1_score', linestyle = '-', color = 'gray')
  plt.legend()
  plt.ylabel('Split Vs ROC')
  plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
def metrics(y_true, y_preds, y_probs):
    roc = roc_auc_score(y_true,y_probs)
    print("The ROC AUC Score is: {}".format(roc))
    cf = confusion_matrix(y_true,y_preds)
    print("The Confusion Matrix is:")
    print(cf)
    f1score = f1_score(y_true, y_preds)
    print("The F1 score is: {}".format(f1score))
    fpr,tpr,_ = roc_curve(y_true,y_preds)
    print("ROC_AUC curve")
    pyplot.plot(fpr, tpr, marker='.', label='ROC_AUC')

l = [p_2011,
     p_2012,
     p_2013,
     p_2014,
     p_2015]
Wi = 'Winner'
rocs = []
average = 0
comb_y_true = []
comb_y_preds = []
comb_y_probs = []
for i in range(4):
    frames = []
    for j in range(i+1):
        frames.append(l[j])
    # print("\n")
    train = pd.concat(frames)
    X_train = train.drop([Wi], axis = 1)
    scaler = StandardScaler()
    scaler.fit(X_train)  
    X_train = scaler.transform(X_train)
    y_train = train[Wi]
    clf = DecisionTreeClassifier(random_state=0, criterion="entropy", ccp_alpha=0.06,
                                 class_weight='balanced',
                                 splitter = "random")
    clf.fit(X_train, y_train)
    test = l[i+1]
    X_test = test.drop([Wi], axis = 1)
    X_test = scaler.transform(X_test)
    y_test = test[Wi]
    y_preds = clf.predict(X_test)
    y_probs = clf.predict_proba(X_test)[:,1]
    comb_y_true.append(y_test)
    comb_y_preds.append(y_preds)
    comb_y_probs.append(y_probs)
    metrics(y_test, y_preds, y_probs)
metrics_combined(comb_y_true, comb_y_preds, comb_y_probs)

plot_pruning_graph(X_train, y_train, clf)